{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re \n",
    "import string\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaTokenizerFast, RobertaModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : \n",
    "\n",
    "    - Check Acronyms \n",
    "    - Avoid Overfitting on Dataset names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Chunking Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coleridge_df = pd.read_csv('./data/dataset/coleridge_df.csv', index_col = 0)\n",
    "coleridge_test = pd.read_csv('./data/dataset/coleridge_test.csv', index_col = 0)\n",
    "coleridge_df = coleridge_df.rename({'body':'document', 'dataset_label' : 'answer'})\n",
    "coleridge_test = coleridge_test.rename({'body':'document'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>pub_title</th>\n",
       "      <th>dataset_title</th>\n",
       "      <th>dataset_label</th>\n",
       "      <th>cleaned_label</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d0fa7568-7d8e-4db9-870f-f9c6f668c17b</td>\n",
       "      <td>The Impact of Dual Enrollment on College Degre...</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>national education longitudinal study</td>\n",
       "      <td>This study used data from the National Educati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2f26f645-3dec-485d-b68d-f013c9e05e60</td>\n",
       "      <td>Educational Attainment of High School Dropouts...</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>national education longitudinal study</td>\n",
       "      <td>Dropping out of high school is not necessarily...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c5d5cd2c-59de-4f29-bbb1-6a88c7b52f29</td>\n",
       "      <td>Differences in Outcomes for Female and Male St...</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>national education longitudinal study</td>\n",
       "      <td>, stress satisfactory outcomes for all youth,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5c9a3bc9-41ba-4574-ad71-e25c1442c8af</td>\n",
       "      <td>Stepping Stone and Option Value in a Model of ...</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>national education longitudinal study</td>\n",
       "      <td>Federal Reserve Bank of Richmond S1. Accountin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c754dec7-c5a3-4337-9892-c02158475064</td>\n",
       "      <td>Parental Effort, School Resources, and Student...</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>national education longitudinal study</td>\n",
       "      <td>This article investigates an important factor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19656</th>\n",
       "      <td>b3498176-8832-4033-aea6-b5ea85ea04c4</td>\n",
       "      <td>RSNA International Trends: A Global Perspectiv...</td>\n",
       "      <td>RSNA International COVID-19 Open Radiology Dat...</td>\n",
       "      <td>RSNA International COVID Open Radiology Database</td>\n",
       "      <td>rsna international covid open radiology database</td>\n",
       "      <td>Our lives have been fundamentally altered this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19657</th>\n",
       "      <td>f77eb51f-c3ac-420b-9586-cb187849c321</td>\n",
       "      <td>MCCS: a novel recognition pattern-based method...</td>\n",
       "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
       "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
       "      <td>cas covid 19 antiviral candidate compounds dat...</td>\n",
       "      <td>The outbreak of the coronavirus disease 2019 ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19658</th>\n",
       "      <td>ab59bcdd-7b7c-4107-93f5-0ccaf749236c</td>\n",
       "      <td>Quantitative Structure–Activity Relationship M...</td>\n",
       "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
       "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
       "      <td>cas covid 19 antiviral candidate compounds dat...</td>\n",
       "      <td>The ongoing COVID-19 pandemic has challenged t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19659</th>\n",
       "      <td>fd23e7e0-a5d2-4f98-992d-9209c85153bb</td>\n",
       "      <td>A ligand-based computational drug repurposing ...</td>\n",
       "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
       "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
       "      <td>cas covid 19 antiviral candidate compounds dat...</td>\n",
       "      <td>deployment of approximative mathematical model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19660</th>\n",
       "      <td>fd23e7e0-a5d2-4f98-992d-9209c85153bb</td>\n",
       "      <td>A ligand-based computational drug repurposing ...</td>\n",
       "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
       "      <td>CAS COVID-19 antiviral candidate compounds data</td>\n",
       "      <td>cas covid 19 antiviral candidate compounds data</td>\n",
       "      <td>deployment of approximative mathematical model...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19661 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Id  \\\n",
       "0      d0fa7568-7d8e-4db9-870f-f9c6f668c17b   \n",
       "1      2f26f645-3dec-485d-b68d-f013c9e05e60   \n",
       "2      c5d5cd2c-59de-4f29-bbb1-6a88c7b52f29   \n",
       "3      5c9a3bc9-41ba-4574-ad71-e25c1442c8af   \n",
       "4      c754dec7-c5a3-4337-9892-c02158475064   \n",
       "...                                     ...   \n",
       "19656  b3498176-8832-4033-aea6-b5ea85ea04c4   \n",
       "19657  f77eb51f-c3ac-420b-9586-cb187849c321   \n",
       "19658  ab59bcdd-7b7c-4107-93f5-0ccaf749236c   \n",
       "19659  fd23e7e0-a5d2-4f98-992d-9209c85153bb   \n",
       "19660  fd23e7e0-a5d2-4f98-992d-9209c85153bb   \n",
       "\n",
       "                                               pub_title  \\\n",
       "0      The Impact of Dual Enrollment on College Degre...   \n",
       "1      Educational Attainment of High School Dropouts...   \n",
       "2      Differences in Outcomes for Female and Male St...   \n",
       "3      Stepping Stone and Option Value in a Model of ...   \n",
       "4      Parental Effort, School Resources, and Student...   \n",
       "...                                                  ...   \n",
       "19656  RSNA International Trends: A Global Perspectiv...   \n",
       "19657  MCCS: a novel recognition pattern-based method...   \n",
       "19658  Quantitative Structure–Activity Relationship M...   \n",
       "19659  A ligand-based computational drug repurposing ...   \n",
       "19660  A ligand-based computational drug repurposing ...   \n",
       "\n",
       "                                           dataset_title  \\\n",
       "0                  National Education Longitudinal Study   \n",
       "1                  National Education Longitudinal Study   \n",
       "2                  National Education Longitudinal Study   \n",
       "3                  National Education Longitudinal Study   \n",
       "4                  National Education Longitudinal Study   \n",
       "...                                                  ...   \n",
       "19656  RSNA International COVID-19 Open Radiology Dat...   \n",
       "19657  CAS COVID-19 antiviral candidate compounds dat...   \n",
       "19658  CAS COVID-19 antiviral candidate compounds dat...   \n",
       "19659  CAS COVID-19 antiviral candidate compounds dat...   \n",
       "19660  CAS COVID-19 antiviral candidate compounds dat...   \n",
       "\n",
       "                                           dataset_label  \\\n",
       "0                  National Education Longitudinal Study   \n",
       "1                  National Education Longitudinal Study   \n",
       "2                  National Education Longitudinal Study   \n",
       "3                  National Education Longitudinal Study   \n",
       "4                  National Education Longitudinal Study   \n",
       "...                                                  ...   \n",
       "19656   RSNA International COVID Open Radiology Database   \n",
       "19657  CAS COVID-19 antiviral candidate compounds dat...   \n",
       "19658  CAS COVID-19 antiviral candidate compounds dat...   \n",
       "19659  CAS COVID-19 antiviral candidate compounds dat...   \n",
       "19660    CAS COVID-19 antiviral candidate compounds data   \n",
       "\n",
       "                                           cleaned_label  \\\n",
       "0                  national education longitudinal study   \n",
       "1                  national education longitudinal study   \n",
       "2                  national education longitudinal study   \n",
       "3                  national education longitudinal study   \n",
       "4                  national education longitudinal study   \n",
       "...                                                  ...   \n",
       "19656   rsna international covid open radiology database   \n",
       "19657  cas covid 19 antiviral candidate compounds dat...   \n",
       "19658  cas covid 19 antiviral candidate compounds dat...   \n",
       "19659  cas covid 19 antiviral candidate compounds dat...   \n",
       "19660    cas covid 19 antiviral candidate compounds data   \n",
       "\n",
       "                                                    body  \n",
       "0      This study used data from the National Educati...  \n",
       "1      Dropping out of high school is not necessarily...  \n",
       "2       , stress satisfactory outcomes for all youth,...  \n",
       "3      Federal Reserve Bank of Richmond S1. Accountin...  \n",
       "4      This article investigates an important factor ...  \n",
       "...                                                  ...  \n",
       "19656  Our lives have been fundamentally altered this...  \n",
       "19657  The outbreak of the coronavirus disease 2019 ,...  \n",
       "19658  The ongoing COVID-19 pandemic has challenged t...  \n",
       "19659  deployment of approximative mathematical model...  \n",
       "19660  deployment of approximative mathematical model...  \n",
       "\n",
       "[19661 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coleridge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_misspells_answer(document, answer) :\n",
    "    \n",
    "    # Handle special case by hand\n",
    "    if 'Alzheimer' in document :\n",
    "        document = document.replace('\\'s', 's')\n",
    "        \n",
    "    if 'NOAA' in document : \n",
    "        document = document.replace('NOAA / National', 'NOAA National')\n",
    "        document = document.replace('water-level', 'water level')\n",
    "        document = document.replace('NOAA / National Water Level Observation Network',  'NOAA National Water Level Observation Network')\n",
    "        \n",
    "    if 'Baltimore Longitudinal Study of Aging' in document : \n",
    "        document = document.replace('Baltimore Longitudinal Study of Aging, BLSA', 'Baltimore Longitudinal Study of Aging (BLSA)')\n",
    "        document = document.replace('Baltimore Longitudinal Study of Aging (BLSA; represented below the gene)', 'Baltimore Longitudinal Study of Aging (BLSA)')\n",
    "        document = document.replace('Baltimore Longitudinal Study of Aging (BLSA-3T)', 'Baltimore Longitudinal Study of Aging (BLSA)')\n",
    "        document = document.replace('BALTIMORE LONGITUDINAL STUDY OF AGING', 'Baltimore Longitudinal Study of Aging (BLSA)')\n",
    "    \n",
    "    reg = answer.replace('(', '\\(', )\n",
    "    reg = reg.replace(')', '\\)', )\n",
    "    \n",
    "    # Clean punctuation surrounding dataset name\n",
    "    re1 = '.' + reg + '.'\n",
    "    document = re.sub(re1, ' '+answer+' ', document)\n",
    "    \n",
    "    # Case sensitive\n",
    "    document = re.sub(reg, answer, document, flags = re.I)\n",
    "    \n",
    "    # Accronym punctued cleaning\n",
    "    accr = answer.split()[0]\n",
    "    document = re.sub('.'+accr+'.', ' '+accr+' ', document)\n",
    "    \n",
    "    # Replace double whitespaces\n",
    "    document = document.replace('  ', ' ')\n",
    "    \n",
    "    return document\n",
    "\n",
    "def clean_text(text):\n",
    "    punct = string.punctuation.replace(\"'\",\"\")\n",
    "    text = re.sub('[%s]' % re.escape(punct), ' ', str(text).lower())\n",
    "    text = re.sub('[\\d]', ' ', text)\n",
    "    text = re.sub(' [\\w]{1} ', ' ', text)\n",
    "    text = re.sub('\\s+([a-zA-Z]\\s+)*', ' ', text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def find_answer(document, answer) :\n",
    "    document = document.split()\n",
    "    answer = answer.split()\n",
    "    \n",
    "    all_positions = []\n",
    "    for i in range(len(document)-len(answer)+1) : \n",
    "        flag = False\n",
    "        if document[i:i+len(answer)] == answer : \n",
    "            flag = True\n",
    "            \n",
    "        if flag == True : \n",
    "            all_positions.append([i,i+len(answer)])\n",
    "                \n",
    "    return all_positions\n",
    "\n",
    "def chunking_documents(documents, answers, tokenizer, max_seq_length, sliding_window) : \n",
    "    \n",
    "    documents_chunked = []\n",
    "    aug_answers = []\n",
    "    all_answers_pos = []\n",
    "    for document, answer in tqdm(zip(documents, answers)) : \n",
    "        \n",
    "        document = replace_misspells_answer(document, answer)\n",
    "        \n",
    "        input_ids = tokenizer.encode(document, add_special_tokens = False)\n",
    "        document_length = len(input_ids)\n",
    "        \n",
    "        chunks, chunks_answers_pos, chunk_ans = [], [], []\n",
    "        for i in range(0, math.ceil((document_length - max_seq_length) / sliding_window) + 1): \n",
    "            chunk = input_ids[i*sliding_window:(i*sliding_window) + max_seq_length]\n",
    "            \n",
    "            chunk_doc = tokenizer.decode(chunk)\n",
    "            answer_pos = find_answer(clean_text(chunk_doc), clean_text(answer))\n",
    "            \n",
    "            if answer_pos == [] : \n",
    "                chunk_ans.append('')\n",
    "                \n",
    "            else : \n",
    "                chunk_ans.append(answer)\n",
    "            \n",
    "            chunks.append(chunk_doc)\n",
    "            chunks_answers_pos.append(answer_pos)\n",
    "            \n",
    "        all_answers_pos.append(chunks_answers_pos)\n",
    "        documents_chunked.append(chunks)\n",
    "        n_chunks = len(chunks)\n",
    "        aug_answer = [answer] * n_chunks\n",
    "        aug_answers.append(aug_answer)\n",
    "        \n",
    "    return documents_chunked, aug_answers, all_answers_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990c5505b89f441886e199c1d8b064c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2072 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "documents = coleridge_df.body.to_numpy()\n",
    "answers = coleridge_df.dataset_label.to_numpy()\n",
    "\n",
    "max_seq_length = 490\n",
    "overlapping_size = 50\n",
    "sliding_window = max_seq_length - overlapping_size\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "\n",
    "documents_chunked, answers, answers_pos = chunking_documents(documents, answers, tokenizer, max_seq_length, sliding_window)\n",
    "\n",
    "chunked_dataset = {\n",
    "    'document' : documents_chunked,\n",
    "    'answer' : answers,\n",
    "    'answer_pos' : answers_pos\n",
    "}\n",
    "\n",
    "chk_coleridge_df = pd.DataFrame(chunked_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This study used data from the National Educati...</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>[[6, 10]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Appendix D for more information). The study a...</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>collected in 2000. For a more detailed descri...</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>[[71, 75], [103, 107]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>family income and prior achievement. In respo...</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the improvement index represents the gain or ...</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19660</th>\n",
       "      <td>to evaluate compound similarity by quantifyin...</td>\n",
       "      <td>CAS COVID-19 antiviral candidate compounds data</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19660</th>\n",
       "      <td>used as an input. For example, remdesivir was...</td>\n",
       "      <td>CAS COVID-19 antiviral candidate compounds data</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19660</th>\n",
       "      <td>(2 unique Murcko scaffolds) with activity &lt; 1...</td>\n",
       "      <td>CAS COVID-19 antiviral candidate compounds data</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19660</th>\n",
       "      <td>COVID-19 pandemic, this course was conceptual...</td>\n",
       "      <td>CAS COVID-19 antiviral candidate compounds data</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19660</th>\n",
       "      <td>repurposing strategies in the framework of th...</td>\n",
       "      <td>CAS COVID-19 antiviral candidate compounds data</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>526813 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  \\\n",
       "0      This study used data from the National Educati...   \n",
       "0       Appendix D for more information). The study a...   \n",
       "0       collected in 2000. For a more detailed descri...   \n",
       "0       family income and prior achievement. In respo...   \n",
       "0       the improvement index represents the gain or ...   \n",
       "...                                                  ...   \n",
       "19660   to evaluate compound similarity by quantifyin...   \n",
       "19660   used as an input. For example, remdesivir was...   \n",
       "19660   (2 unique Murcko scaffolds) with activity < 1...   \n",
       "19660   COVID-19 pandemic, this course was conceptual...   \n",
       "19660   repurposing strategies in the framework of th...   \n",
       "\n",
       "                                                answer              answer_pos  \n",
       "0                National Education Longitudinal Study               [[6, 10]]  \n",
       "0                National Education Longitudinal Study                      []  \n",
       "0                National Education Longitudinal Study  [[71, 75], [103, 107]]  \n",
       "0                National Education Longitudinal Study                      []  \n",
       "0                National Education Longitudinal Study                      []  \n",
       "...                                                ...                     ...  \n",
       "19660  CAS COVID-19 antiviral candidate compounds data                      []  \n",
       "19660  CAS COVID-19 antiviral candidate compounds data                      []  \n",
       "19660  CAS COVID-19 antiviral candidate compounds data                      []  \n",
       "19660  CAS COVID-19 antiviral candidate compounds data                      []  \n",
       "19660  CAS COVID-19 antiviral candidate compounds data                      []  \n",
       "\n",
       "[526813 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chk_coleridge_df = chk_coleridge_df.explode(['document', 'answer', 'answer_pos'])\n",
    "chk_coleridge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_has_answer(answer_pos) : \n",
    "    if len(answer_pos) == 0 :\n",
    "        return False \n",
    "    else : \n",
    "        return True\n",
    "    \n",
    "chk_coleridge_df['has_answer'] = chk_coleridge_df['answer_pos'].apply(check_has_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(chk_coleridge_df, open('./data/dataset/chunked_coleridge_df.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking_documents_inference(documents, tokenizer, max_seq_length, sliding_window) : \n",
    "    \n",
    "    documents_chunked = []\n",
    "    for document in tqdm(documents) : \n",
    "        \n",
    "        input_ids = tokenizer.encode(document, add_special_tokens = False)\n",
    "        document_length = len(input_ids)\n",
    "        \n",
    "        chunks = []\n",
    "        for i in range(0, math.ceil((document_length - max_seq_length) / sliding_window) + 1): \n",
    "            chunk = input_ids[i*sliding_window:(i*sliding_window) + max_seq_length]\n",
    "            \n",
    "            chunk_doc = tokenizer.decode(chunk)\n",
    "            \n",
    "            chunks.append(chunk_doc)\n",
    "            \n",
    "        documents_chunked.append(chunks)\n",
    "        \n",
    "    return documents_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc59396dd404eeeb76dc14681b31144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5285 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cognitive deficits and reduced educational ach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>performance is associated with lower educatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>with completion of college (rs11584700 and rs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>et al., 2013) The next (and ongoing) wave of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>have genotypes for the SNPs of interest were ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grains, poultry and cured meats. In this sect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>characteristics and locations of our sample o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>account for some of the differences in our fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shoppers in the NE region. We may also be wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of IRI data finds a smaller share of LI house...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             document\n",
       "0   Cognitive deficits and reduced educational ach...\n",
       "0    performance is associated with lower educatio...\n",
       "0    with completion of college (rs11584700 and rs...\n",
       "0    et al., 2013) The next (and ongoing) wave of ...\n",
       "0    have genotypes for the SNPs of interest were ...\n",
       "..                                                ...\n",
       "3    grains, poultry and cured meats. In this sect...\n",
       "3    characteristics and locations of our sample o...\n",
       "3    account for some of the differences in our fi...\n",
       "3    shoppers in the NE region. We may also be wit...\n",
       "3    of IRI data finds a smaller share of LI house...\n",
       "\n",
       "[150 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = coleridge_test.body.to_numpy()\n",
    "\n",
    "max_seq_length = 490\n",
    "overlapping_size = 50\n",
    "sliding_window = max_seq_length - overlapping_size\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "\n",
    "test_documents_chunked = chunking_documents_inference(documents, tokenizer, max_seq_length, sliding_window)\n",
    "\n",
    "test_chunked_dataset = {\n",
    "    'document' : test_documents_chunked,\n",
    "}\n",
    "\n",
    "test_chk_coleridge_df = pd.DataFrame(test_chunked_dataset)\n",
    "test_chk_coleridge_df = test_chk_coleridge_df.explode(['document'])\n",
    "test_chk_coleridge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(test_chk_coleridge_df, open('./data/dataset/test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Splitting Train & dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_coleridge_df = pkl.load(open('./data/dataset/chunked_coleridge_df.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer = chk_coleridge_df['answer'].unique()\n",
    "has_answer = chk_coleridge_df['has_answer'].to_numpy()\n",
    "\n",
    "train_answer, dev_answer, _, _ = train_test_split(answer, answer, test_size = 0.2)\n",
    "train_answer, test_answer, _, _ = train_test_split(train_answer, train_answer, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts that contain an answer : 29057\n",
      "Number of texts that do not contain an answer : 319156\n"
     ]
    }
   ],
   "source": [
    "train = chk_coleridge_df[chk_coleridge_df.answer.isin(train_answer)]\n",
    "print(f'Number of texts that contain an answer : {len(train[train.has_answer == True])}')\n",
    "print(f'Number of texts that do not contain an answer : {len(train[train.has_answer == False])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts that contain an answer : 6819\n",
      "Number of texts that do not contain an answer : 110384\n"
     ]
    }
   ],
   "source": [
    "dev = chk_coleridge_df[chk_coleridge_df.answer.isin(dev_answer)]\n",
    "print(f'Number of texts that contain an answer : {len(dev[dev.has_answer == True])}')\n",
    "print(f'Number of texts that do not contain an answer : {len(dev[dev.has_answer == False])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts that contain an answer : 3536\n",
      "Number of texts that do not contain an answer : 57861\n"
     ]
    }
   ],
   "source": [
    "test = chk_coleridge_df[chk_coleridge_df.answer.isin(test_answer)]\n",
    "print(f'Number of texts that contain an answer : {len(test[test.has_answer == True])}')\n",
    "print(f'Number of texts that do not contain an answer : {len(test[test.has_answer == False])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(train, open('./data/dataset/train.pkl', 'wb'))\n",
    "pkl.dump(dev, open('./data/dataset/dev.pkl', 'wb'))\n",
    "pkl.dump(test, open('./data/dataset/test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Downsampling FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pkl.load(open('./data/dataset/train.pkl', 'rb'))\n",
    "dev = pkl.load(open('./data/dataset/dev.pkl', 'rb'))\n",
    "test = pkl.load(open('./data/dataset/test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsampling_FN(dataframe) : \n",
    "    \n",
    "    reset_df = dataframe.reset_index()\n",
    "    FN = reset_df[reset_df.has_answer == False]\n",
    "    TP = reset_df[reset_df.has_answer == True]\n",
    "    \n",
    "    n_true = len(TP)\n",
    "    n_false = len(FN)\n",
    "    \n",
    "    keep_ratio = 1- (n_true/n_false)\n",
    "    \n",
    "    keep = [False if np.random.random_sample() < keep_ratio else True for k in range(n_false)]\n",
    "    \n",
    "    keep_index = FN.index.to_numpy()[keep]\n",
    "    keep_index = np.concatenate((keep_index,TP.index.to_numpy()))\n",
    "    \n",
    "    downsample_df = reset_df[reset_df.index.isin(keep_index)]\n",
    "    downsample_df.index = downsample_df['index']\n",
    "    downsample_df = downsample_df.drop(columns = ['index'])\n",
    "    \n",
    "    return downsample_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = downsampling_FN(train)\n",
    "dev_ds = downsampling_FN(dev)\n",
    "test_ds = downsampling_FN(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(train_ds, open('./data/dataset/train_ds.pkl', 'wb'))\n",
    "pkl.dump(dev_ds, open('./data/dataset/dev_ds.pkl', 'wb'))\n",
    "pkl.dump(test_ds, open('./data/dataset/test_ds.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
